{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-06-26 13:11:32+00:00</td>\n",
       "      <td>1673318075465121792</td>\n",
       "      <td>Dus wij sturen wapens en geld naar de Oekraïne...</td>\n",
       "      <td>PMG_RIP</td>\n",
       "      <td>https://twitter.com/PMG_RIP/status/16733180754...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-26 13:11:31+00:00</td>\n",
       "      <td>1673318072550039553</td>\n",
       "      <td>Lumineon V $9 \\nPWE $1 \\nBMWT $3 https://t.co/...</td>\n",
       "      <td>TattooedPokemon</td>\n",
       "      <td>https://twitter.com/TattooedPokemon/status/167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-26 13:11:31+00:00</td>\n",
       "      <td>1673318072457871360</td>\n",
       "      <td>Loodswezen Antwerpen wordt culinair centrum 23...</td>\n",
       "      <td>de_beyn</td>\n",
       "      <td>https://twitter.com/de_beyn/status/16733180724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-06-26 13:11:31+00:00</td>\n",
       "      <td>1673318072365744129</td>\n",
       "      <td>nvm, sis is here &amp;gt;&amp;lt; https://t.co/vgojxqaiAK</td>\n",
       "      <td>alx_yeonie</td>\n",
       "      <td>https://twitter.com/alx_yeonie/status/16733180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-26 13:11:31+00:00</td>\n",
       "      <td>1673318071446953991</td>\n",
       "      <td>Mijn therapeut vond mijn jurk mooi vandaag ☺️</td>\n",
       "      <td>roosferrero</td>\n",
       "      <td>https://twitter.com/roosferrero/status/1673318...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   Datetime             Tweet Id  \\\n",
       "0           0  2023-06-26 13:11:32+00:00  1673318075465121792   \n",
       "1           1  2023-06-26 13:11:31+00:00  1673318072550039553   \n",
       "2           2  2023-06-26 13:11:31+00:00  1673318072457871360   \n",
       "3           3  2023-06-26 13:11:31+00:00  1673318072365744129   \n",
       "4           4  2023-06-26 13:11:31+00:00  1673318071446953991   \n",
       "\n",
       "                                                Text         Username  \\\n",
       "0  Dus wij sturen wapens en geld naar de Oekraïne...          PMG_RIP   \n",
       "1  Lumineon V $9 \\nPWE $1 \\nBMWT $3 https://t.co/...  TattooedPokemon   \n",
       "2  Loodswezen Antwerpen wordt culinair centrum 23...          de_beyn   \n",
       "3  nvm, sis is here &gt;&lt; https://t.co/vgojxqaiAK       alx_yeonie   \n",
       "4      Mijn therapeut vond mijn jurk mooi vandaag ☺️      roosferrero   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://twitter.com/PMG_RIP/status/16733180754...  \n",
       "1  https://twitter.com/TattooedPokemon/status/167...  \n",
       "2  https://twitter.com/de_beyn/status/16733180724...  \n",
       "3  https://twitter.com/alx_yeonie/status/16733180...  \n",
       "4  https://twitter.com/roosferrero/status/1673318...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helpers import tokenize_tweet\n",
    "from tqdm import tqdm\n",
    "result_dir = \"results\"\n",
    "tweets = pd.read_csv(os.path.join(result_dir, \"10k_tweets_nl.csv\"), index_col=False)\n",
    "tweets.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_up(word) -> str:\n",
    "    word = word.lower()\n",
    "\n",
    "    # Skip accounts\n",
    "    if word.startswith(\"@\"):\n",
    "        return \"\"\n",
    "    # Skip urls\n",
    "    if word.startswith(r\"https://\"):\n",
    "        return \"\"\n",
    "    \n",
    "    if word.startswith(\"#\"):\n",
    "        return word[1:]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = {}\n",
    "\n",
    "for tweet in tweets.Text:\n",
    "    words = tokenize_tweet(tweet)\n",
    "    for word in words:\n",
    "        word = cleaning_up(word)\n",
    "        # Skip emtpy strings\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        all_words[word] = all_words.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 10000 tweets we found 28660 unique words\n"
     ]
    }
   ],
   "source": [
    "print(f\"In {len(tweets)} tweets we found {len(all_words)} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurrence</th>\n",
       "      <th>percentage</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>5053</td>\n",
       "      <td>0.033690</td>\n",
       "      <td>0.033690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>van</td>\n",
       "      <td>2886</td>\n",
       "      <td>0.019242</td>\n",
       "      <td>0.052932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>het</td>\n",
       "      <td>2675</td>\n",
       "      <td>0.017835</td>\n",
       "      <td>0.070767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>2655</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.088469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>2654</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>0.106164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>een</td>\n",
       "      <td>2642</td>\n",
       "      <td>0.017615</td>\n",
       "      <td>0.123779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>is</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.013335</td>\n",
       "      <td>0.137114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>op</td>\n",
       "      <td>1635</td>\n",
       "      <td>0.010901</td>\n",
       "      <td>0.148015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>voor</td>\n",
       "      <td>1443</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.157636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ik</td>\n",
       "      <td>1408</td>\n",
       "      <td>0.009388</td>\n",
       "      <td>0.167024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>met</td>\n",
       "      <td>1320</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.175825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dat</td>\n",
       "      <td>1293</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.184446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>je</td>\n",
       "      <td>1258</td>\n",
       "      <td>0.008388</td>\n",
       "      <td>0.192834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>te</td>\n",
       "      <td>1157</td>\n",
       "      <td>0.007714</td>\n",
       "      <td>0.200548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>niet</td>\n",
       "      <td>1143</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.208169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>die</td>\n",
       "      <td>1032</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.215050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>zijn</td>\n",
       "      <td>870</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>0.220851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>maar</td>\n",
       "      <td>721</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0.225658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>om</td>\n",
       "      <td>694</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.230285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>aan</td>\n",
       "      <td>689</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.234879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>er</td>\n",
       "      <td>681</td>\n",
       "      <td>0.004540</td>\n",
       "      <td>0.239419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>als</td>\n",
       "      <td>638</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.243673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>dit</td>\n",
       "      <td>628</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.247860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>bij</td>\n",
       "      <td>618</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.251980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>wat</td>\n",
       "      <td>606</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.256020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  occurrence  percentage  cumulative\n",
       "7      de        5053    0.033690    0.033690\n",
       "24    van        2886    0.019242    0.052932\n",
       "110   het        2675    0.017835    0.070767\n",
       "4      en        2655    0.017702    0.088469\n",
       "18     in        2654    0.017695    0.106164\n",
       "112   een        2642    0.017615    0.123779\n",
       "57     is        2000    0.013335    0.137114\n",
       "14     op        1635    0.010901    0.148015\n",
       "71   voor        1443    0.009621    0.157636\n",
       "85     ik        1408    0.009388    0.167024\n",
       "155   met        1320    0.008801    0.175825\n",
       "23    dat        1293    0.008621    0.184446\n",
       "246    je        1258    0.008388    0.192834\n",
       "25     te        1157    0.007714    0.200548\n",
       "116  niet        1143    0.007621    0.208169\n",
       "103   die        1032    0.006881    0.215050\n",
       "186  zijn         870    0.005801    0.220851\n",
       "147  maar         721    0.004807    0.225658\n",
       "159    om         694    0.004627    0.230285\n",
       "32    aan         689    0.004594    0.234879\n",
       "164    er         681    0.004540    0.239419\n",
       "115   als         638    0.004254    0.243673\n",
       "271   dit         628    0.004187    0.247860\n",
       "183   bij         618    0.004120    0.251980\n",
       "389   wat         606    0.004040    0.256020"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_df = pd.DataFrame(all_words.items(), columns=[\"word\", \"occurrence\"])\n",
    "number_of_words = np.sum(all_words_df.occurrence)\n",
    "all_words_df.reset_index(drop=True)\n",
    "all_words_df = all_words_df.sort_values(\"occurrence\", ascending=False)\n",
    "all_words_df[\"percentage\"] = np.round(all_words_df.occurrence / number_of_words, 6)\n",
    "all_words_df[\"cumulative\"] = all_words_df.percentage.cumsum()\n",
    "all_words_df.to_csv(os.path.join(result_dir, \"all_words.csv\"), index=False)\n",
    "all_words_df.head(n=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy lemma word count\n",
    "A lot of the words are filler, we want nouns, verbs, and adjectives. For this we use spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "model_name = \"nl_core_news_lg\"\n",
    "try:\n",
    "    nlp = spacy.load(model_name)\n",
    "except OSError:\n",
    "    print(f\"downloading the model {model_name}\")\n",
    "    spacy.cli.download(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mijn mijn PRON VNW|bez|det|stan|vol|1|ev|prenom|zonder|agr nmod:poss Xxxx True True\n",
      "appel appel NOUN N|soort|ev|basis|zijd|stan nsubj xxxx True False\n",
      "is zijn AUX WW|pv|tgw|ev cop xx True True\n",
      "op op ADP VZ|init case xx True True\n",
      "de de DET LID|bep|stan|rest det xx True True\n",
      "tafel tafel NOUN N|soort|ev|basis|zijd|stan ROOT xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mijn appel is op de tafel\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mijn mijn PRON VNW|bez|det|stan|vol|1|ev|prenom|zonder|agr ROOT Xxxx True True\n",
      "# # PUNCT LET punct # False False\n",
      "appels appel NOUN N|soort|mv|basis nsubj xxxx True False\n",
      "is zijn AUX WW|pv|tgw|ev cop xx True True\n",
      "op op ADP VZ|init case xx True True\n",
      "de de DET LID|bep|stan|rest det xx True True\n",
      "tafel tafel NOUN N|soort|ev|basis|zijd|stan ROOT xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mijn #appels is op de tafel\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:01<00:00, 162.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 10000 tweets we found 26269 unique lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_lemmas = {}\n",
    "\n",
    "for tweet in tqdm(tweets.Text):\n",
    "    words = tokenize_tweet(tweet)\n",
    "    doc = nlp(\" \".join(words))\n",
    "\n",
    "    for token in doc:\n",
    "        # Don't break up hashtags\n",
    "        if token.text.startswith(\"#\"):\n",
    "           continue\n",
    "\n",
    "        lemma = token.lemma_.lower()\n",
    "        all_lemmas[lemma] = all_lemmas.get(lemma, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 10000 tweets we found 26269 unique lemma\n"
     ]
    }
   ],
   "source": [
    "print(f\"In {len(tweets)} tweets we found {len(all_lemmas)} unique lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurrence</th>\n",
       "      <th>percentage</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>5090</td>\n",
       "      <td>0.033911</td>\n",
       "      <td>0.033911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>zijn</td>\n",
       "      <td>3544</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>0.057522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>van</td>\n",
       "      <td>2886</td>\n",
       "      <td>0.019227</td>\n",
       "      <td>0.076749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>2675</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.094571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>het</td>\n",
       "      <td>2675</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.112393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>2657</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.130095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>een</td>\n",
       "      <td>2593</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.147370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>op</td>\n",
       "      <td>1635</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.158263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>voor</td>\n",
       "      <td>1457</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.167970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ik</td>\n",
       "      <td>1408</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.177351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>met</td>\n",
       "      <td>1320</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.186145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dat</td>\n",
       "      <td>1293</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.194759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>je</td>\n",
       "      <td>1258</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.203140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>te</td>\n",
       "      <td>1236</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>0.211375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>niet</td>\n",
       "      <td>1143</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.218990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>die</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.006929</td>\n",
       "      <td>0.225919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hebben</td>\n",
       "      <td>1039</td>\n",
       "      <td>0.006922</td>\n",
       "      <td>0.232841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>veel</td>\n",
       "      <td>743</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.237791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>worden</td>\n",
       "      <td>723</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.242608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>maar</td>\n",
       "      <td>721</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.247412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>om</td>\n",
       "      <td>694</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.252036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>aan</td>\n",
       "      <td>689</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.256626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>er</td>\n",
       "      <td>683</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.261176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>als</td>\n",
       "      <td>638</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.265427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kunnen</td>\n",
       "      <td>631</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.269631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  occurrence  percentage  cumulative\n",
       "7        de        5090    0.033911    0.033911\n",
       "57     zijn        3544    0.023611    0.057522\n",
       "24      van        2886    0.019227    0.076749\n",
       "4        en        2675    0.017822    0.094571\n",
       "107     het        2675    0.017822    0.112393\n",
       "18       in        2657    0.017702    0.130095\n",
       "109     een        2593    0.017275    0.147370\n",
       "14       op        1635    0.010893    0.158263\n",
       "70     voor        1457    0.009707    0.167970\n",
       "83       ik        1408    0.009381    0.177351\n",
       "151     met        1320    0.008794    0.186145\n",
       "23      dat        1293    0.008614    0.194759\n",
       "238      je        1258    0.008381    0.203140\n",
       "25       te        1236    0.008235    0.211375\n",
       "112    niet        1143    0.007615    0.218990\n",
       "101     die        1040    0.006929    0.225919\n",
       "97   hebben        1039    0.006922    0.232841\n",
       "161    veel         743    0.004950    0.237791\n",
       "44   worden         723    0.004817    0.242608\n",
       "143    maar         721    0.004804    0.247412\n",
       "155      om         694    0.004624    0.252036\n",
       "32      aan         689    0.004590    0.256626\n",
       "160      er         683    0.004550    0.261176\n",
       "111     als         638    0.004251    0.265427\n",
       "15   kunnen         631    0.004204    0.269631"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lemmas_df = pd.DataFrame(all_lemmas.items(), columns=[\"word\", \"occurrence\"])\n",
    "number_of_lemmas = np.sum(all_lemmas_df.occurrence)\n",
    "all_lemmas_df.reset_index(drop=True)\n",
    "all_lemmas_df = all_lemmas_df.sort_values(\"occurrence\", ascending=False)\n",
    "all_lemmas_df[\"percentage\"] = np.round(all_lemmas_df.occurrence / number_of_lemmas, 6)\n",
    "all_lemmas_df[\"cumulative\"] = all_lemmas_df.percentage.cumsum()\n",
    "all_lemmas_df.to_csv(os.path.join(result_dir, \"all_lemmas.csv\"), index=False)\n",
    "all_lemmas_df.head(n=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 777/777 [00:00<?, ?B/s] \n",
      "c:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tom.nijhof\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 990M/990M [15:23<00:00, 1.07MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cabir40/t5-dutch-grammar-correction\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"cabir40/t5-dutch-grammar-correction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Mn kat zit op de grond.</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Mn kat zitten op de gront.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]c:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 12/10000 [00:30<6:57:10,  2.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m tweet \u001b[39min\u001b[39;00m tqdm(tweets\u001b[39m.\u001b[39mText):\n\u001b[0;32m      4\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(tweet, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m----> 6\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids)\n\u001b[0;32m      7\u001b[0m     corrected_tweets\u001b[39m.\u001b[39mappend(tokenizer\u001b[39m.\u001b[39mdecode(outputs[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\transformers\\generation\\utils.py:1329\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1321\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1322\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1323\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m         )\n\u001b[0;32m   1326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1327\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1329\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m   1330\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[0;32m   1331\u001b[0m     )\n\u001b[0;32m   1333\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\transformers\\generation\\utils.py:642\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    640\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    641\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 642\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[0;32m    644\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1090\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1078\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1079\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     )\n\u001b[0;32m   1089\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1090\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1091\u001b[0m         hidden_states,\n\u001b[0;32m   1092\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1093\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   1094\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1095\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1096\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   1097\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   1098\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   1099\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1100\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1101\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1102\u001b[0m     )\n\u001b[0;32m   1104\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:753\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    750\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[0;32m    752\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> 753\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[0;32m    755\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:342\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m    341\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 342\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[0;32m    343\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[0;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:310\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 310\u001b[0m     hidden_gelu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwi_0(hidden_states))\n\u001b[0;32m    311\u001b[0m     hidden_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi_1(hidden_states)\n\u001b[0;32m    312\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_gelu \u001b[39m*\u001b[39m hidden_linear\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tom.nijhof\\AppData\\Local\\miniconda3\\envs\\lang\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corrected_tweets = []\n",
    "\n",
    "for tweet in tqdm(tweets.Text[:30]):\n",
    "    input_ids = tokenizer(tweet, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(input_ids)\n",
    "    corrected_tweets.append(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> Dus wij sturen wapens en geld naar de Oekraïne om vervolgens de North',\n",
       " '<pad> $9 PWE $1 BMWT $3 ht',\n",
       " '<pad> Brugge verraste 26/6/2023.</s>',\n",
       " '<pad> &gt; https://t.co/',\n",
       " '<pad> Mijn therapeut vond mijn jurk mooi vandaag ☺️.</s>',\n",
       " '<pad> Tegenwoordig worden namen van componisten tegenwoordig voor de raarste dingen gebruikt.',\n",
       " '<pad> Ik haat jullie zo erg!</s>',\n",
       " '<pad> 3 https://t.co/KAha6',\n",
       " '<pad> DRAGGENIUS.</s>',\n",
       " '<pad> Ik heb nogsteeds mn weetabix die ik voor ramadan',\n",
       " '<pad> Geitenbaard (Aruncus) #watgroeit ',\n",
       " '<pad> De lach van de euro https://t.co/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
